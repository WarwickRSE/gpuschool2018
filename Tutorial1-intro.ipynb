{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming Summer School\n",
    "D. Quigley, University of Warwick\n",
    "## Tutorial 1. Using CUDA accelerated libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Sanity check of Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All being well, you've reading this text inside a Jupyter notebook environment running on an SCRTP machine equipped with a CUDA-compatible GPU card and have launched the notebook from within an environment configured to support GPU computing. If in doubt, check the [connecting instructions](https://warwick.ac.uk/fac/sci/maths/research/events/2017-18/nonsymposium/gpu/connecting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some simpple checks to make sure you can execute code on a GPU from within this notebook. For today we'll be working with the Python interface to CUDA provided via the numba package.\n",
    "\n",
    "Some terminology we need aleady:\n",
    "\n",
    "**Host**        : The traditional computer in which our code is running on a CPU with access to host RAM.\n",
    "\n",
    "**CUDA Device** : The GPU card consisting of its own RAM and computing cores (lots of them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "This notebook is running on  brigitte.csc.warwick.ac.uk\n",
      "========================================================\n",
      "Device ID :  0  :  Tesla K20c\n",
      "Device ID :  1  :  NVS 310\n"
     ]
    }
   ],
   "source": [
    "import platform          # So we can figure out where we're running\n",
    "from numba import cuda   # Import python interface to CUDA\n",
    "\n",
    "# Report where we're running\n",
    "print(\"========================================================\")\n",
    "print(\"This notebook is running on \", platform.node())\n",
    "print(\"========================================================\")\n",
    "\n",
    "# Test if CUDA is available. If so report on the devices present \n",
    "if cuda.is_available():  \n",
    "    \n",
    "    # List of CUDA capable devices in this system\n",
    "    for device in cuda.list_devices():       \n",
    "        print(\"Device ID : \", device.id, \" : \", device.name.decode())         \n",
    "    \n",
    "else:\n",
    "    print(\"There doesn't appear to be a CUDA capable device in this system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the the most appropriate device and query its [compute capability](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities). CUDA devices have varying compute capability depending on the product range and when they were manufactured. Typically CUDA software will be written to support a minimum compute capability.\n",
    "\n",
    "[Numba requires a compute capability of 2.0 or higher](https://numba.pydata.org/numba-doc/dev/cuda/overview.html), so we should check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device ( Tesla K20c ) has a sufficient compute capability\n"
     ]
    }
   ],
   "source": [
    "my_instance = cuda.select_device(0) # Create a device instance to work with based on device 0 \n",
    "\n",
    "# The compute capability is stored as a tuple (major, minor) so we're good to go if...\n",
    "if my_instance.compute_capability[0] >= 2:\n",
    "    print(\"The selected device (\",my_instance.name.decode(),\") has a sufficient compute capability\")\n",
    "else:\n",
    "    print(\"The selected device does not have a sufficient compute capability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : \"Drop in\" replacements for standard numerical libraries via Pyculib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computationally intensive part of many scientific codes reduces to standard numerical operations, e.g. manipulation of large matrices/vectors, performing Fourier transforms, dealing with sparse linear algebra etc.\n",
    "\n",
    "In the traditional High Performance Computing (HPC) realm of compiled C/C++/Fortran code, there exists a suite of standard optimised libraries for such things. The CUDA toolkit provides GPU-enabled versions of these, and numba provides Python interfaces to these through the pyculib package which we'll experiment with below.\n",
    "\n",
    "The advantage of this approach is that no real knowledge of GPU programming is required, we simply replace calls to standard CPU functions with GPU-accelerated equivalents. The disadvantage is that we only accelerate part of the code, and may suffer performance overheads associated with transering data between host and device every time we use one of the functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2(a) cuBLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAS is the suite of [Basic Linear Algebra Subprograms](http://www.netlib.org/blas/). These come in three levels, for both real and complex data types. \n",
    "\n",
    "**Level 1** : Vector-vector operations \n",
    "\n",
    "**Level 2** : Matrix-vector operations\n",
    "\n",
    "**Level 3** : Matrix-matrix operations\n",
    "\n",
    "On any well-managed HPC system, the local installations of numpy and scipy packages will be built on top of BLAS routines (written in C or Fortran) that have been optimised for the particular hardware in use. Optimised BLAS implementations for CPUs include [OpenBLAS](https://www.openblas.net/), [Atlas](http://math-atlas.sourceforge.net/), [Intel MKL](https://software.intel.com/en-us/mkl) and [AMD ACML](https://developer.amd.com/building-with-acml/).\n",
    "\n",
    "The CUDA toolkit include [cuBLAS](https://developer.nvidia.com/cublas), a GPU-accelerated BLAS implementation. Let's compare how this performs in comparison to numpy. If you're interested, the numpy implementation on the SCRTP desktops has been built using OpenBLAS, but optimised only for the most common Intel CPU features to ensure compatibility. CPU performace will not be great.\n",
    "\n",
    "[Documentation for the pyculib interface to cuBLAS](http://pyculib.readthedocs.io/en/latest/cublas.html)\n",
    "\n",
    "Let's illustrate this with a simple matrix-matrix multiplication example. The BLAS routine `dgemm` (double precision, general matrix-matrix multiply) performs the following operation.\n",
    "\n",
    "$$ C = \\alpha AB + \\beta C $$\n",
    "\n",
    "where $A$, $B$ and $C$ are matrices and $\\alpha$ and $\\beta$ are scalars. Other specialised routines are available for matrices with particular structure (e.g. banded, tri-diagonal, symmetric) but we won't worry about that today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyculib.blas as cublas      # Python interface to cuBLAS, cuFFT, cuSPARSE and cuRAND\n",
    "\n",
    "# Set size of matrix to work with\n",
    "size = 3\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "A = np.random.rand(size,size)\n",
    "B = np.random.rand(size,size)\n",
    "\n",
    "# Alpha and beta\n",
    "alpha = 0.5\n",
    "beta = 2.0\n",
    "\n",
    "# Perform the operation described above using standard numpy\n",
    "C_np = alpha * np.matmul(A,B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gemm in module pyculib.blas:\n",
      "\n",
      "gemm(transa, transb, alpha, A, B, beta=0, C=None, stream=None)\n",
      "    Generalized matrix-matrix multiplication:\n",
      "    \n",
      "    C <- alpha*transa(A)*transb(B) + beta*C\n",
      "    \n",
      "    'beta' and 'C' are optional on input. Return 'C'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query what we need to pass into cublas.gemm\n",
    "help(cublas.gemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equivalent cuBLAS call needs additional arguments to specify if we want to use the\n",
    "# matrices as supplied ('N' - no operation) or their transpose ('T'). We'll ignore streams\n",
    "# for now but might come back to that later if time permits. Arguments for beta and C are optional.\n",
    "C_cu = cublas.gemm('N', 'N', alpha, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot just happend behind the scenes there. The numpy arrays have been copied from the host RAM into the device memory, the matrix operation has been performed using the CUDA cores on the device, and the result has been copied back into the numpy array C_cu in the host memory.\n",
    "\n",
    "Let's be proper programmers and check that both computations gave the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,  -6.93889390e-18],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract the two results\n",
    "C_cu - C_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to investigate is whether this cuBLAS function is any faster. Let's try a more substantial matrix and time how long numpy takes to perform the same operation as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using numpy took :  92.39946100569796  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "import time   # For timing \n",
    "\n",
    "size = 1000\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "A = np.random.rand(size,size)\n",
    "B = np.random.rand(size,size)\n",
    "\n",
    "# What is the time before we start the operation?\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# Perform the operation described above using standard numpy\n",
    "C_np = alpha * np.matmul(A,B) \n",
    "\n",
    "# What is the time after we finish the operation in milliseconds?\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using numpy took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To time how long cuBLAS takes to do the same thing we'll use its own timer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using cuBLAS took :  35.06553649902344  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# Create two CUDA events\n",
    "event1 = cuda.event(timing=True)\n",
    "event2 = cuda.event(timing=True)\n",
    "\n",
    "# First event before we call gemm\n",
    "event1.record()\n",
    "\n",
    "# Do the multiplication\n",
    "C_cu = cublas.gemm('N', 'N', alpha, A, B)\n",
    "\n",
    "# Second event after we call gemm\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using cuBLAS took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this using the K20c in brigitte.csc.warwick.ac.uk the GPU was a factor of three faster for 1000x1000 matrices. We'll explore how this scales with matrix size using the GPUs on Tinis a little later.\n",
    "\n",
    "Recall that `cublas.gemm` is copying data to and from the GPU as well as performing the calculation. We can seperate this out into smaller steps to understand where the time is being spent.\n",
    "\n",
    "First, let's explicitly copy the arrays into the device memory. Note the convention that arrays which live in the device memory are given variable names prefixed with `d_`. For reference in this section see [Section 3.3 of the documentation on Numba for CUDA GPUs](https://numba.pydata.org/numba-doc/dev/cuda/memory.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating device arrays took :  6.630015850067139  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "event1.record() # First event\n",
    "\n",
    "# Create new device arrays - first copy A and B\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "\n",
    "# and create a new array on the device to hold C (faster than copying an empty array from the host)\n",
    "d_C = cuda.device_array((size,size))\n",
    "\n",
    "# We haven't done anything with these arrays yet. The program may continue past this point\n",
    "# while the arrays are copying and only wait for the copy to finish at the point when\n",
    "# we need to use them, i.e. the copy is *asynchronous*. The following ensures the operation is\n",
    "# complete before we record the next timing event.\n",
    "cuda.synchronize()\n",
    "\n",
    "event2.record() # Second event\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Creating device arrays took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python interface we're using is clever enough to understand that when we pass device arrays as arguments to cuBLAS functions it should use those instead of copying data over from the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using cuBLAS took :  38.06185531616211  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# First event before we call gemm\n",
    "event1.record()\n",
    "\n",
    "# Do the multiplication\n",
    "cublas.gemm('N', 'N', alpha, d_A, d_B)\n",
    "\n",
    "# Second event after we call gemm\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using cuBLAS took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might now want to copy the result stored in the device array back onto the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying output back to host memory took :  7.132800102233887  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# First event before we transfer the data\n",
    "event1.record()\n",
    "\n",
    "C_cu = d_C.copy_to_host()\n",
    "cuda.synchronize()\n",
    "\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Copying output back to host memory took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll likely see that the total time taken in the three steps above is a bit longer than when we let the cuBLAS routine handle the copying behind the scenes due to the enforced synchronisation.\n",
    "\n",
    "The take home point here is that time spent copying data to and from the device takes time. Best performance will be acheived by doing as much work as possible on the arrays while they are on the device, minimising copies between host and device memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2(a) Exercises\n",
    "\n",
    "You should now have everything you need to canabalise the code snippets above and construct some python scripts which you can run outside of this notebook environment on the *Tinis GPU node*. **Please do not run matrices larger than 1000x1000 in this notebook**. Questions to explore:\n",
    "\n",
    "* How large a matrix is needed before the multiplication is 10x faster on the GPU compared to numpy code? \n",
    "* What is the largest speedup you can obtain?\n",
    "* How does the speedup from BLAS level 1 and 2 routines compare to BLAS level 3?\n",
    "* What is the variability in your timings, what gives rise to this?\n",
    "* (Advanced) Is the numpy code making use of multiple CPU cores on the Tinis node?\n",
    "* (Advanced) How does the cuBLAS/numpy performance compare to using scipy.linalg.blas.dgemm?\n",
    "* (Very Advanced) How does the shape of the matrix change performance. Why?\n",
    "\n",
    "Tutors will be available during the session to help you answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2(a) cuFFT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
