{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to GPU Programming Summer School\n",
    "========================\n",
    "\n",
    "D. Quigley, University of Warwick\n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial 2: Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a kernel?\n",
    "\n",
    "* #### A function which operates on an element of data and is executed by a CUDA thread\n",
    "* #### We launch enough threads such that the operation is performed on all elements\n",
    "* #### Threads are mapped onto our data via a grid of thread blocks\n",
    "* #### Each instance of the function (thread) must be able to identify its location in the grid \n",
    "* #### Kernels are launched/invoked by the host, but run on the GPU\\*\n",
    "\n",
    "(* Newer version of CUDA allow kernels to launch kernels, but numba doesn't support this yet)\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel limitations\n",
    "\n",
    "* #### Can only return data via arguments to the kernel function - no return value\n",
    "* #### Kernels cannot perform input or output - no printing or reading/writing files\n",
    "* #### Exception handling inside kernels is limited (no ```try```/```catch```)\n",
    "* #### Only a subset of the host language (e.g. Python or C) is supported\n",
    "* #### Threads execute in lockstep within *warps* of 32 threads which map onto *multiprocessors* (groups of CUDA cores, e.g. 8, 32 or 128, 192 etc depending on the architecture version)\n",
    "\n",
    "\n",
    "The final limitation might seem unimportant, but this means that anything which causes one thread in a warp to wait (e.g. for data to arrive from memory) will cause all threads to wait. Similary branches, (if statements) are problematic. Each thread must execute both branches and then decide which result to keep. This may subvert traditional expectations of how to optimise code.\n",
    "\n",
    "---\n",
    "\n",
    "## Thread blocks/grids\n",
    "\n",
    "* #### Threads within a block can make use of some *shared device memory* - more on that in tutorial 3\n",
    "* #### All threads can read/write to global device memory. This is where all our device arrays have been so far\n",
    "* #### There are hardware limitations on the number of threads per block\n",
    "* #### The grid can be 1D, 2D or 3D\n",
    "\n",
    "See https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html for supported features. In particular note that numpy functions which dynamically create new arrays are not allowed in kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial Example\n",
    "\n",
    "This all makes more sense with an example. \n",
    "\n",
    "Let's start with the kernel we need to improve the porg example from Tutorial 1. Recall that we want to multiply each element in a 2D matrix by the corresponding element in another matrix. This suggests we use a 2D grid of threads with one thread per matrix element.\n",
    "\n",
    "Our kernel looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def multiply_elements(a, b, c):\n",
    "    \"\"\"\n",
    "    Element-wise multiplication of a and b stored in c.\n",
    "    \"\"\"\n",
    "\n",
    "    # What elements of a,b and c should this thread operate on?\n",
    "    tx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    ty = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "\n",
    "    # Better make sure the indices tx adn ty are inside the array!\n",
    "    if ty < a.shape[0] and tx < a.shape[1]:\n",
    "        c[ty, tx] = a[ty, tx] * b[ty, tx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've *decorated* this function with the identifier ```@cuda.jit``` which requires some explanation. Numba contains functionality to turn python functions into complied GPU code when first invoked. This is known as \"just In time compilation\" and will be familiar to Julia fans. Note that numba can also \"jit\" functions which run on the host (CPU) which might be useful if wanting to make fair CPU vs GPU benchmarks.\n",
    "\n",
    "\n",
    "\n",
    "The next part of the function uses variables defined for us by CUDA which give each thread (i.e. instance of the function) a unique element of the thread grid to operate on...\n",
    "\n",
    "\n",
    "```python\n",
    "cuda.threadIdx.x  # Index of this thread within its block (x - direction)\n",
    "cuda.blockIdx.x   # Which block of threads is this (x - direction)\n",
    "cuda.blockDim.x   # Number of threads in each block (x - direction)\n",
    "```\n",
    "\n",
    "... plus similar in the y direction (and z if in 3D). To get the global position inside the grid we perform the computation \n",
    "\n",
    "```python\n",
    "tx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "```\n",
    "\n",
    "i.e. add the thread index within the current block thread to the number of threads in all previous blocks.\n",
    "\n",
    "Finally we check that the resulting indices `tx` and `ty` will have something to operate on by comparing them to corresponding array sizes before addressing the arrays to perform the desired computation **on this single element **. Note that I'm being sloppy here and assuming somebody else has made sure `a`, `b` and `c` are all the same size/shape.\n",
    "\n",
    "\n",
    "```python\n",
    "if ty < a.shape[0] and tx < a.shape[1]:\n",
    "        c[ty,tx] = a[ty,tx] * b[ty,tx]\n",
    "```\n",
    "\n",
    "This may seem unnecessary but is essential good practice. We have to use a whole number of blocks in the grid, which can often mean we launch more threads than necessary. The extra threads shouldn't try to write to memory which lies outside of the array or they'd be overwriting other data which we might need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to know how to launch the kernel. Start with a trivial example using a 3x3 matrix for both of the inputs arrays such that the output should just contain the squares of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array for the input data and copy it to the device\n",
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float)\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Create an array for the output\n",
    "d_c = cuda.device_array((3, 3),dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now launch one thread per element, passing the *device* arrays to the kernel function\n",
    "multiply_elements[(1, 1), (3, 3)](d_a, d_a, d_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function call looks like a normal python function call, but the arguments are prefixed by a 2-element list which specifies the number of blocks in the grid (first element) and the threads per block (second argument). For our 2D array each element in the list is a tuple specifying the size in each grid direction.\n",
    "\n",
    "Here I've used one block of 3x3 threads mapping onto my 3x3 matrices. Lets check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data back from the device to the host\n",
    "c = d_c.copy_to_host()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoorah!\n",
    "\n",
    "I might equally well have used 3x3 blocks of 1 thread each, but for bigger problems there are some things to think about.\n",
    "\n",
    "* Threads are organised into warps of 32 - block sizes which are not a multiple of 32 will end up wasting some of a multiprocessor\n",
    "* It might be desirable to maximise block size and hence the number of threads with access to the same *shared memory* \n",
    "* Each device has a maximum number of threads allowed per block. This can be queried in CUDA C codes but for our purposes we can check [the wikipedia page on CUDA](https://en.wikipedia.org/wiki/CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revenge of the Porg\n",
    "\n",
    "Let's return to our image example from tutorial 1 and roughly (i.e. inside the notebook) the time taken for the whole convolution.\n",
    "\n",
    "First we need to re-read the image and create the 2D Gaussian we want to convolve with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image        # Import the Python Image Library\n",
    "import pyculib.fft as cufft  # Import the cuFFT library interface\n",
    "from timeit import default_timer as timer  # Timer\n",
    "\n",
    "# Open an image file and convert to single colour (greyscale)\n",
    "img = Image.open('porg.jpg').convert('L')\n",
    "img_data = np.asarray(img,dtype=float)\n",
    "dim = img_data.shape[0]\n",
    "#dim = 250\n",
    "#img_resized = img.resize((dim,dim))\n",
    "\n",
    "# Define the Gaussian to volume with\n",
    "width = 0.2\n",
    "domain = np.linspace(-5, 5,dim)\n",
    "gauss = np.exp(-0.5*domain**2/(width*width)) \n",
    "shift = int(dim/2)\n",
    "gauss = np.roll(gauss,shift)\n",
    "gauss2D = gauss[:,np.newaxis] * gauss[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all the arrays we need and move the input data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data complex\n",
    "img_data_complex = img_data + 1j * np.zeros((dim,dim))\n",
    "gauss2D_complex = gauss2D + 1j * np.zeros((dim,dim))\n",
    "\n",
    "# Arrays to store intermediate result and final output on host\n",
    "img_fft = np.empty((dim,dim),dtype=complex)\n",
    "gauss_fft = np.empty((dim,dim),dtype=complex)\n",
    "img_ifft = np.empty((dim,dim),dtype=complex)\n",
    "\n",
    "# Put the data on the device\n",
    "d_img_data_complex = cuda.to_device(img_data_complex)\n",
    "d_gauss2D_complex = cuda.to_device(gauss2D_complex)\n",
    "\n",
    "# Create device arrays\n",
    "d_img_fft = cuda.device_array((dim,dim),dtype=np.complex)\n",
    "d_gauss_fft = cuda.device_array((dim,dim),dtype=np.complex)\n",
    "d_img_ifft = cuda.device_array((dim,dim),dtype=np.complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the convolution the way we did it yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = timer()\n",
    "\n",
    "# FFT the two input arrays\n",
    "cufft.fft(d_img_data_complex,d_img_fft)\n",
    "cufft.fft(d_gauss2D_complex,d_gauss_fft)\n",
    "\n",
    "# Copy data back to host\n",
    "img_fft=d_img_fft.copy_to_host()\n",
    "gauss_fft = d_gauss_fft.copy_to_host()\n",
    "\n",
    "# Multiply each element in fft_img by the corresponding image in fft_gaus\n",
    "img_conv = img_fft * gauss_fft\n",
    "\n",
    "# Copy to the device\n",
    "d_img_conv = cuda.to_device(img_conv)\n",
    "\n",
    "# Inverse Fourier transform\n",
    "cufft.ifft(d_img_conv,d_img_ifft)\n",
    "        \n",
    "# Copy result back to host\n",
    "img_ifft = d_img_ifft.copy_to_host()\n",
    "\n",
    "t2 = timer()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "print(\"Convolution with multiplication on host took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the convolution using our new kernel, i.e. avoiding the need to copy data back to the host for the intermediate multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = timer()  # Start timer\n",
    "\n",
    "# FFT the two input arrays\n",
    "cufft.fft(d_img_data_complex, d_img_fft)\n",
    "cufft.fft(d_gauss2D_complex, d_gauss_fft)\n",
    "\n",
    "# Use the kernel to multiply on the device\n",
    "threads_per_block = (32, 32)\n",
    "blocks = dim // 32 + 1\n",
    "blocks_per_grid = (blocks, blocks)\n",
    "\n",
    "d_img_conv = cuda.device_array((dim, dim),dtype=np.complex)\n",
    "\n",
    "multiply_elements[blocks_per_grid, threads_per_block](d_img_fft, d_gauss_fft, d_img_conv)\n",
    "\n",
    "# Inverse Fourier transform\n",
    "cufft.ifft(d_img_conv, d_img_ifft)\n",
    "        \n",
    "# Copy result back to host\n",
    "img_ifft = d_img_ifft.copy_to_host()\n",
    "\n",
    "t2 = timer()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "print(\"Convolution with multiplication on device took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see that that is slower than our old method.\n",
    "\n",
    "BUT - remember that our kernel is compiled on first use so this slower time include the time taken to compile the kernel. Run the above cell again and you should see that there's a significant improvement and that avoiding the intermediate memory transfers between host and device has saved us a huge amount of time!\n",
    "\n",
    "Let's check the porg is OK and appropriately blurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Show the porg\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(img_ifft.real,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Cut and paste from the above to create a script which benchmarks the two methods of performing the convolution. Gather timings on the Tinis GPU node as an average over 10 runs. Remember to exclude the first run of the kernel-based method from your average if you want to measure only execution time.\n",
    "\n",
    "* How much faster than the orignal CPU-based numpy implementation is this version?\n",
    "\n",
    "* (Advanced) Can the CPU version be made faster by 'jitting' a function for the multiplication?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandelbrot set\n",
    "\n",
    "The kernel we've created above is about as trivial as it gets. They can be much more involved. As an example let's look at generation of the [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set). I'm borrowing very heavily here from an example workbook on GitHub, so full credit for this example goes to the original author.\n",
    "\n",
    "https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb\n",
    "\n",
    "As well as demonstrating that kernels can be less trivial, this also demonstrates the concept of a *device function*, i.e. a function which will only ever run on the GPU and will only be called from within a kernel. It cannot be called from the host.\n",
    "\n",
    "The coordinates $x$, $y$ are part of the Mandelbrot set if the iterative map\n",
    "\n",
    "$$ z_{i+1} = z_{i}^{2} + c $$\n",
    "\n",
    "does not diverge when the complex numbers $z_{0} = 0$ and $ c = x + iy$. To make things graphical we colour pixels in the $x$, $y$ plane according to how rapidly this map diverges, i.e. how many iterations it takes the magnitude of $z$ to reach some threshold value.\n",
    "\n",
    "The following function counts this number of iterations for a threshold value of 4 for input coordinates $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import autojit\n",
    "#@autojit\n",
    "def mandel(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this stands `mandel` is a standard python function which will run on the CPU. We could copy and paste this into a new function `mandel_cpu` decorated with\n",
    "\n",
    "```python\n",
    "@cuda.jit(Device=True)\n",
    "```\n",
    "\n",
    "to indicate that it will be a device function. Numba provides an easier way to create device functions from standard python functions however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the device function mandel_gpu from the function \"mandel\" above\n",
    "mandel_gpu = cuda.jit(device=True)(mandel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need is a kernel which calls this function for each point in the $x$, $y$ plane. Some note on this...\n",
    "\n",
    "* We calculate the pixel size on every thread, duplicating effort. The original author of this example may be assuming this is faster than calculating it once on the CPU and suffering an extra copy to device memory but you might want to experiment with that.\n",
    "\n",
    "\n",
    "* Each instance of the kernel (thread) operates on a single element of the image array as in the porg example. For very large images this might not be practical. A more complicated kernel would operate on a chunk of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mandel_kernel(min_x, max_x, min_y, max_y, image, iters):\n",
    "  \n",
    "  # Get the dimensions of the grid from the image device array\n",
    "  dimx = image.shape[1]\n",
    "  dimy = image.shape[0]\n",
    "\n",
    "  # Work out spacing between elements \n",
    "  pixel_size_x = (max_x - min_x) / dimx\n",
    "  pixel_size_y = (max_y - min_y) / dimy\n",
    "\n",
    "  # What elements of the image should this thread operate on?\n",
    "  tx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "  ty = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "\n",
    "  # Coordinates in the complex plane\n",
    "  real = min_x + tx * pixel_size_x\n",
    "  imag = min_y + ty * pixel_size_y \n",
    "    \n",
    "  # Count number of interations needed to diverge\n",
    "  if ty < dimy and tx < dimx:\n",
    "      image[ty, tx] = mandel_gpu(real, imag, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must create an image array, and specify what range we want this to represent along the real and imaginary axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to hold the output image - i.e. number of iterations \n",
    "# as an unsigned 8 bit integer\n",
    "image = np.zeros((1000, 1500), dtype = np.uint8)\n",
    "\n",
    "# Range over which we want to explore membership of the set\n",
    "rmin = -1.2475 ; rmax = -1.2450\n",
    "imin = 0.400 ; imax = 0.41\n",
    "\n",
    "# Maximum number of iterations before deciding \"does not diverge\"\n",
    "maxits = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's time how long it takes to populate the image on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = timer() # Start timer\n",
    "\n",
    "pixel_size_x = (rmax - rmin) / image.shape[1]\n",
    "pixel_size_y = (imax - imin) / image.shape[0]\n",
    "\n",
    "# This is probably the most non-pythonic way to do this...\n",
    "for j in range(image.shape[0]):\n",
    "    for i in range(image.shape[1]):\n",
    "        \n",
    "        real = rmin + i * pixel_size_x\n",
    "        imag = imin + j * pixel_size_y\n",
    "        \n",
    "        image[j, i] = mandel(real, imag, maxits)\n",
    "        \n",
    "t2 = timer()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Mandelbot created on CPU in : \",1000*(t2-t1),\" milliseconds.\")\n",
    "\n",
    "# Display the image\n",
    "#plt.figure(figsize = [9, 9])\n",
    "#plt.imshow(image,cmap='RdBu',extent=[rmin, rmax, imin, imax]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use our kernel to populate the image on the GPU. Note that I'm fixing the number of threads per block at 32 x 32, and then deciding how many blocks to launch from this and the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image size above is chosen to map onto a whole number of threadblocks. \n",
    "# IMPORTANT - we normally think of arrays indexed as row, column hence y, x\n",
    "# The tuples specifiying the thread grid dimensions are indexed as x, y\n",
    "threads_per_block = (32, 32) \n",
    "\n",
    "bx = image.shape[1] // threads_per_block[1] + 1\n",
    "by = image.shape[0] // threads_per_block[0] + 1\n",
    "\n",
    "blocks_per_grid = (bx, by)\n",
    "\n",
    "t1 = timer() # Start timer\n",
    "\n",
    "# Copy image to a device array which we will populate in our kernel\n",
    "d_image = cuda.to_device(image)\n",
    "\n",
    "# Launch the kernel, passing the range of x and y to use \n",
    "mandel_kernel[blocks_per_grid, threads_per_block](rmin, rmax, imin, imax, d_image, maxits) \n",
    "\n",
    "# Copy the resulting image back to the host\n",
    "image = d_image.copy_to_host()\n",
    "\n",
    "t2 = timer()  # Stop timer\n",
    "\n",
    "print(\"Mandelbot created on GPU in : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you'll have to run this more than one to see how fast it executes once the kernel has already been compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plt.figure(figsize = [9, 9])\n",
    "plt.imshow(image,cmap='RdBu',extent=[rmin, rmax, imin, imax]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* The speedup obtained from using the GPU in this example is likely too good to be true (and is). By 'jitting' the CPU `mandel` function can you measure a more realistic speedup? HINT: see the comments where `mandel` is first defined.\n",
    "\n",
    "* Cut and paste from the above to create a script which benchmarks the GPU vs CPU implemenation as a function of image resolution (size of the numpy array `image`). Run this on the Tinis GPU ndoes. How does speedup vary with problem size.\n",
    "\n",
    "* (Advanced) Are the choices made for `blocks_per_grid` and `threads_per_block` optimal? You may need to generate rather large images to see any variation with these quantities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
