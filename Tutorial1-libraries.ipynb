{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to GPU Programming Summer School\n",
    "========================\n",
    "\n",
    "D. Quigley, University of Warwick\n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial 1: GPU Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computationally intensive part of many scientific codes reduces to standard numerical operations, e.g. manipulation of large matrices/vectors, performing Fourier transforms, dealing with sparse linear algebra etc.\n",
    "\n",
    "In the traditional High Performance Computing (HPC) realm of compiled C/C++/Fortran code, there exists a suite of standard optimised libraries for such things. The CUDA toolkit provides GPU-enabled versions of these, and numba provides Python interfaces to these through the pyculib package which we'll experiment with below.\n",
    "\n",
    "The advantage of this approach is that no real knowledge of GPU programming is required, we simply replace calls to standard CPU functions with GPU-accelerated equivalents. The disadvantage is that we only accelerate part of the code, and may suffer performance overheads associated with transering data between host and device every time we use one of the functions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuBLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAS is the suite of [Basic Linear Algebra Subprograms](http://www.netlib.org/blas/). These come in three levels, for both real and complex data types. \n",
    "\n",
    "**Level 1** : Vector-vector operations \n",
    "\n",
    "**Level 2** : Matrix-vector operations\n",
    "\n",
    "**Level 3** : Matrix-matrix operations\n",
    "\n",
    "On any well-managed HPC system, the local installations of numpy and scipy packages will be built on top of BLAS routines (written in C or Fortran) that have been optimised for the particular hardware in use. Optimised BLAS implementations for CPUs include [OpenBLAS](https://www.openblas.net/), [Atlas](http://math-atlas.sourceforge.net/), [Intel MKL](https://software.intel.com/en-us/mkl) and [AMD ACML](https://developer.amd.com/building-with-acml/).\n",
    "\n",
    "The CUDA toolkit includes [cuBLAS](https://developer.nvidia.com/cublas), a GPU-accelerated BLAS implementation. Let's compare how this performs in comparison to numpy. If you're interested, the numpy implementation on the SCRTP desktops has been built using OpenBLAS, but optimised only for the most common Intel CPU features to ensure compatibility. CPU performace will not be great.\n",
    "\n",
    "[Documentation for the pyculib interface to cuBLAS](http://pyculib.readthedocs.io/en/latest/cublas.html)\n",
    "\n",
    "Let's illustrate this with a simple matrix-matrix multiplication example. The BLAS routine `dgemm` (double precision, general matrix-matrix multiply) performs the following operation.\n",
    "\n",
    "$$ C = \\alpha AB + \\beta C $$\n",
    "\n",
    "where $A$, $B$ and $C$ are matrices and $\\alpha$ and $\\beta$ are scalars. Other specialised routines are available for matrices with particular structure (e.g. banded, tri-diagonal, symmetric) but we won't worry about that today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import pyculib.blas as cublas      # Python interface to cuBLAS, cuFFT, cuSPARSE and cuRAND\n",
    "\n",
    "# Set size of matrix to work with\n",
    "size = 3\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "A = np.random.rand(size,size)\n",
    "B = np.random.rand(size,size)\n",
    "\n",
    "# Alpha (we'll leave beta as zero)\n",
    "alpha = 0.5\n",
    "\n",
    "# Perform the operation described above using standard numpy\n",
    "C_np = alpha * np.matmul(A,B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gemm in module pyculib.blas:\n",
      "\n",
      "gemm(transa, transb, alpha, A, B, beta=0, C=None, stream=None)\n",
      "    Generalized matrix-matrix multiplication:\n",
      "    \n",
      "    C <- alpha*transa(A)*transb(B) + beta*C\n",
      "    \n",
      "    'beta' and 'C' are optional on input. Return 'C'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query what we need to pass into cublas.gemm\n",
    "help(cublas.gemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two arguments  specify if we want to use the matrices as supplied ('N' - no operation) or their transpose ('T'). We'll ignore streams for now but might come back to that later if time permits. Arguments for beta and C are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the multiplication using the CPU and store the results\n",
    "C_cu = cublas.gemm('N', 'N', alpha, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot just happend behind the scenes there. The numpy arrays have been copied from the host RAM into the device memory, the matrix operation has been performed using the CUDA cores on the device, and the result has been copied back into the numpy array C_cu in the host memory. The device arrays have then been deleted.\n",
    "\n",
    "Let's be proper programmers and check that both computations gave the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match!\n"
     ]
    }
   ],
   "source": [
    "if  (C_cu - C_np).any() > np.finfo(float).eps : \n",
    "    print(\"Results match!\")\n",
    "else:\n",
    "    print(\"Results do not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to investigate is whether this cuBLAS function is any faster. Let's try a more substantial matrix and time how long numpy takes to perform the same operation as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using numpy took :  92.78670698404312  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "import time   # For timing \n",
    "\n",
    "size = 1000\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "A = np.random.rand(size,size)\n",
    "B = np.random.rand(size,size)\n",
    "\n",
    "# What is the time before we start the operation?\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# Perform the operation described above using standard numpy\n",
    "C_np = alpha * np.matmul(A,B) \n",
    "\n",
    "# What is the time after we finish the operation in milliseconds?\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using numpy took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To time how long cuBLAS takes to do the same thing we'll use its own timer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using cuBLAS took :  24.769535064697266  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# Create two CUDA events\n",
    "event1 = cuda.event(timing=True)\n",
    "event2 = cuda.event(timing=True)\n",
    "\n",
    "# First event before we call gemm\n",
    "event1.record()\n",
    "\n",
    "# Do the multiplication\n",
    "C_cu = cublas.gemm('N', 'N', alpha, A, B)\n",
    "\n",
    "# Second event after we call gemm\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using cuBLAS took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this using the K20c in brigitte.csc.warwick.ac.uk the GPU was a factor of three faster for 1000x1000 matrices. We'll explore how this scales with matrix size using the GPUs on Tinis a little later - timings measured inside Jupyter notebooks tend not to be reliable.\n",
    "\n",
    "Recall that `cublas.gemm` is copying data to and from the GPU as well as performing the calculation. We can seperate this out into smaller steps to understand where the time is being spent.\n",
    "\n",
    "First, let's explicitly copy the arrays into the device memory. Note the convention that arrays which live in the device memory are given variable names prefixed with `d_`. For reference in this section see [Section 3.3 of the documentation on Numba for CUDA GPUs](https://numba.pydata.org/numba-doc/dev/cuda/memory.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating device arrays took :  4.349503993988037  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "event1.record() # First event\n",
    "\n",
    "# Create new device arrays - first copy A and B\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "\n",
    "# and create a new array on the device to hold C (faster than copying an empty array from the host)\n",
    "d_C = cuda.device_array((size,size))\n",
    "\n",
    "#cuda.synchronize()\n",
    "\n",
    "event2.record() # Second event\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Creating device arrays took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python interface we're using is clever enough to understand that when we pass device arrays as arguments to cuBLAS functions it should use those instead of copying data over from the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using cuBLAS took :  41.603233337402344  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# First event before we call gemm\n",
    "event1.record()\n",
    "\n",
    "# Do the multiplication\n",
    "cublas.gemm('N', 'N', alpha, d_A, d_B)\n",
    "\n",
    "# Second event after we call gemm\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using cuBLAS took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might now want to copy the result stored in the device array back onto the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying output back to host memory took :  1.8218560218811035  milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# First event before we transfer the data\n",
    "event1.record()\n",
    "\n",
    "C_cu = d_C.copy_to_host()\n",
    "#cuda.synchronize()\n",
    "\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Copying output back to host memory took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't read too much into these timings - these can be quite unreliable when running inside Jupyter notebooks. We'll generate some proper timings in the exercises below.\n",
    "\n",
    "The take home point is that time spent copying data to and from the device takes time. Best performance will be acheived by doing as much work as possible on the arrays while they are on the device, minimising copies between host and device memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuBLAS exercises\n",
    "\n",
    "You should now have everything you need to canabalise the code snippets above and construct some python scripts which you can run outside of this notebook environment on the *Tinis GPU node*. **Please do not run matrices larger than 1000x1000 in this notebook**. On Tinis you should take timings at several matrix sizes as an average over 10 or so runs.\n",
    "\n",
    "Questions to explore:\n",
    "\n",
    "* How large a matrix is needed before the multiplication is 10x faster on the GPU compared to numpy code? \n",
    "* What is the largest speedup you can obtain?\n",
    "* How does the speedup from BLAS level 1 and 2 routines compare to BLAS level 3?\n",
    "* What is the variability in your timings, what gives rise to this?\n",
    "* (Advanced) Is the numpy code making use of multiple CPU cores on the Tinis node?\n",
    "* (Advanced) How does the cuBLAS/numpy performance compare to using scipy.linalg.blas.dgemm?\n",
    "* (Very Advanced) How does the shape of the matrix change performance. Why?\n",
    "\n",
    "Tutors will be available during the session to help you answer these questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuFFT\n",
    "\n",
    "cuFFT provides GPU-enabled functions to perform the Fast Fourier Transform on 1D, 2D and 3D data. It can be used as a drop-in replacement for (e.g.) numpy.fft routines.\n",
    "\n",
    "[Documentation for the pyculib interface to cuFFT](http://pyculib.readthedocs.io/en/latest/cufft.html)\n",
    "\n",
    "To illustrate how it can be used, we'll look at a trivial image processing example. Let's say we need to obscure the identity of a porg (for GDPR reasons) by blurring an image. We can apply a blur filter by convoluting the image with a 2D Gaussian function.\n",
    "\n",
    "A convolution can be performed with the following steps:\n",
    "\n",
    "1. Fourier transform the image and the Gaussian.\n",
    "2. Multiply each element in the Fourier-transformed image by the corresponding element in the Fourier transformed Gaussian.\n",
    "3. Inverse Fourier transform the result.\n",
    "\n",
    "i.e. a convolution becomes a multiplication in Fourier space. This is illustrated using numpy functions and the Python Image Library (PIL) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pillow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cce594a60a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpillow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m    \u001b[0;31m# Import the Python Image Library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We want to use matplot lib to show images inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pillow'"
     ]
    }
   ],
   "source": [
    "from PIL import Image    # Import the Python Image Library\n",
    "\n",
    "# We want to use matplot lib to show images inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Open an image file and convert to single colour (greyscale)\n",
    "img = Image.open('porg.jpg').convert('L')\n",
    "\n",
    "# For now we'll work with a resized version of this image\n",
    "dim = 250\n",
    "img_resized = img.resize((dim,dim))\n",
    "\n",
    "# ... and convert it into a numpy array of floats\n",
    "img_data = np.asarray(img_resized,dtype=float)\n",
    "\n",
    "# Show the porg\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(img_data,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image taken from https://people.com/movies/star-wars-porgs/ (Ed Miller/2017 LucasFilm Ltd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to convolute this image with a Gaussian, so we need a 2D numpy array \n",
    "# representing a Gaussian function with some blurring width\n",
    "width =  0.2 \n",
    "\n",
    "# Define a Gaussian in 1D on a grid with the same number of points as the image\n",
    "domain = np.linspace(-5,5,dim)\n",
    "gauss = np.exp(-0.5*domain**2/(width*width)) \n",
    "    \n",
    "# Roll this around the 1D boundary so that the Gaussian is centered on grid 0,0\n",
    "shift = int(dim/2)\n",
    "gauss = np.roll(gauss,shift)\n",
    "\n",
    "# Turn into a 2D Gaussian\n",
    "gauss2D = gauss[:,np.newaxis] * gauss[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do the convolution on the CPU using standard numpy functions so we have something to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex arrays to hold the output\n",
    "# (Doing this here means array creation won't be included in timings below)\n",
    "img_fft   = np.empty((dim,dim),dtype=complex)\n",
    "gauss_fft = np.empty((dim,dim),dtype=complex)\n",
    "img_ifft  = np.empty((dim,dim),dtype=complex)\n",
    "\n",
    "# Fourier transform the image and the Gaussian using standard numpy functions\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "img_fft   = np.fft.fft2(img_data)\n",
    "gauss_fft = np.fft.fft2(gauss2D)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Forward fast Fourier Transforms took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to multiple each element in img_fft by the corresponding element in gauss_fft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication in Fourier space\n",
    "img_conv = img_fft*gauss_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the result back to real space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the result back into real space using the inverse transform\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "img_ifft = np.fft.ifft2(img_conv)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Inverse fast Fourier Transform took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the convolution should now be stored in img_ifft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result of \"blurring the porg\"\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(img_ifft.real,cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the arrays automatically created for output\n",
    "img_fft   = None\n",
    "gauss_fft = None\n",
    "img_ifft  = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same operation using the cuFFT library via pyculib. \n",
    "\n",
    "There are two ways of doing this, a single function to perform the FFT or a mechanism to create a 'plan' and then invoke this multiple times to perform forward and inverse transforms. This latter mechanism is intended to be familiar for users of the popular library [FFTW (Fastest Fourier Transform in the West)](http://www.fftw.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyculib.fft as cufft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of writing pyculib doesn't seem to correctly deal with real to complex FFTs\n",
    "# so we add an imaginary part to our data before passing to the GPU routines.\n",
    "img_data_complex = img_data + 1j * np.zeros((dim,dim))\n",
    "gauss2D_complex  = gauss2D  + 1j * np.zeros((dim,dim))\n",
    "\n",
    "# Create new complex arrays to hold the output\n",
    "img_fft   = np.empty((dim,dim),dtype=complex)\n",
    "gauss_fft = np.empty((dim,dim),dtype=complex)\n",
    "img_ifft  = np.empty((dim,dim),dtype=complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass host numpy arrays into `cufft.fft()` then the input is automatically copied to the device before the transforms, and the output is copied back afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need this if using the FFTW-like interface\n",
    "#plan = cufft.FFTPlan((dim,dim),complex,complex)\n",
    "\n",
    "event1.record()\n",
    "\n",
    "# FFTW style\n",
    "#plan.forward(img_data_complex,img_fft)\n",
    "#plan.forward(gauss2D_complex,gauss_fft)\n",
    "\n",
    "# Simple style\n",
    "cufft.fft(img_data_complex,img_fft)\n",
    "cufft.fft(gauss2D_complex,gauss_fft)\n",
    "\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Forward fast Fourier Transforms using cuFFT took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication in Fourier space\n",
    "img_conv = img_fft * gauss_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform back into real space\n",
    "\n",
    "event1.record()\n",
    "\n",
    "# FFTW style\n",
    "#plan.inverse(img_conv,img_ifft)\n",
    "\n",
    "# Simple style\n",
    "cufft.ifft(img_conv,img_ifft)\n",
    "\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "print(\"Inverse fast Fourier Transform using cuFFT took : \",t,\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result of \"blurring the porg\" using the GPU\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(img_ifft.real,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the matrix multiplication example we can seperate the data transfer from the actual Fourier transforms. This will allow us to measure the time taken to perform the transforms seperately from the memory transfer, and (in more involved codes) eliminate unnecessary copies between host and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy input data arrays to the device \n",
    "d_img_data_complex = cuda.to_device(img_data_complex)\n",
    "d_gauss2D_complex = cuda.to_device(gauss2D_complex)\n",
    "\n",
    "# Create new device arrays to hold the output\n",
    "d_img_fft = cuda.device_array((dim,dim),dtype=np.complex)\n",
    "d_gauss_fft = cuda.device_array((dim,dim),dtype=np.complex)\n",
    "\n",
    "# Start the timer\n",
    "event1.record()\n",
    "\n",
    "# Perform the forward FFTs\n",
    "cufft.fft(d_img_data_complex,d_img_fft)\n",
    "cufft.fft(d_gauss2D_complex,d_gauss_fft)\n",
    "        \n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Forward fast Fourier Transforms using cuFFT took : \",t,\" milliseconds.\")\n",
    "    \n",
    "# Copy the result back to the host\n",
    "img_fft=d_img_fft.copy_to_host()\n",
    "gauss_fft = d_gauss_fft.copy_to_host()\n",
    "\n",
    "# Multiply each element in fft_img by the corresponding image in fft_gauss\n",
    "img_conv = img_fft * gauss_fft\n",
    "\n",
    "# Copy the product to the device\n",
    "d_img_conv = cuda.to_device(img_conv)\n",
    "\n",
    "# Create a new device array to hold the result of the FFT\n",
    "d_img_ifft = cuda.device_array((dim,dim),dtype=np.complex)\n",
    "\n",
    "event1.record()\n",
    "\n",
    "# Perform the inverse FFT on the data\n",
    "cufft.ifft(d_img_conv,d_img_ifft)\n",
    "\n",
    "event2.record()\n",
    "\n",
    "# Elapsed time (in milliseconds)\n",
    "t = cuda.event_elapsed_time(event1,event2)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Inverse fast Fourier Transform using cuFFT took : \",t,\" milliseconds.\")\n",
    "\n",
    "# Copy result back to host\n",
    "img_ifft = d_img_ifft.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouldn't it be great if we could perform the multiplication at line 29 on the GPU? That way we'd never have to copy `img_fft` and `gauss_fft` back to the host, or copy `img_host` to the device. The entire convolution would be done on the GPU.\n",
    "\n",
    "We'll do exactly that tomorrow when we learn about writing *kernels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we still goet the correct result\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(img_ifft.real,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuFFT exercises\n",
    "\n",
    "As with the matrix exercises, you  now have everything you need to canabalise the code snippets above and construct some python scripts which you can run outside of this notebook environment on the Tinis GPU node. As before, you should average over several runs to evaluate performance.\n",
    "\n",
    "Things to investigate:\n",
    "\n",
    "* How does the performance of cuFFT compare to numpy as a function of image size? Try using the full 2000x2000 pixel image without resizing.\n",
    "\n",
    "* How much of the time taken by cuFFT is spent in memory transfers between host and device?\n",
    "\n",
    "* (Advanced) Investigate the [Python interface to FFTW](https://github.com/pyFFTW/pyFFTW). How does this perform in comparison to numpy and cuFFT?  Does the GPU only appear fast here because the numpy FFT implementation is sub-optimal? You might need to install pyFFTW using `pip install --user pyfftw` inside the environment you launched the notebook from.\n",
    "\n",
    "* (Very advanced) cuFFT provides the ability to perform multiple FFTs concurrently across multiple GPUs by providing multiple sets of input in batches. Can you GPU accelerate applying a Gaussian filter to the original *colour* image by constructing a batch of FFTS, one for each colour? You might need the [cuFFT documentaion](https://docs.nvidia.com/cuda/cufft/index.html) as well as that for the Python interface.\n",
    "\n",
    "\n",
    "\n",
    "Tutors will be available during the session to help you answer these questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other libraries\n",
    "\n",
    "Many other GPU accelerated libraries exist. For example...\n",
    "\n",
    "* [cuRAND](http://pyculib.readthedocs.io/en/latest/curand.html)      : Part of the Nvidia toolkit. Fast random number generation on GPUs for applications which need large numbers of samples \n",
    "* [cuSPARSE](http://pyculib.readthedocs.io/en/latest/cusparse.html)    : Also part of the Nvidia toolkit. Sparse linear algebra library\n",
    "* [cuDNN](https://developer.nvidia.com/cudnn)       : Nvidia Deep Neural Network Library (used by e.g. [TensorFlow](https://www.tensorflow.org/)) \n",
    "* [libgpuarray](http://deeplearning.net/software/libgpuarray/installation.html) : Another tensor manipulation library with [PyGPU](http://deeplearning.net/software/libgpuarray/pyapi/pygpu.html) as the Python interface. Used by [Theano](http://deeplearning.net/software/theano/) \n",
    "\n",
    "These are all available in the environment we loaded when following the instructions on the connecting page. Feel free to play around with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
