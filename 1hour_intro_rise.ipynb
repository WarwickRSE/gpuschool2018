{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Preliminaries not useful to show in talk. Execute before slideshow.\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "real_range = widgets.FloatRangeSlider(\n",
    "    value=[-2, 1],\n",
    "    min=-2,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Real range:',\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    "    continuous_update=False,\n",
    "    layout=Layout(width='80%', height='30px'),\n",
    ")\n",
    "imag_range = widgets.FloatRangeSlider(\n",
    "    value=[-1, 1],\n",
    "    min=-1,\n",
    "    max=1,\n",
    "    step=0.01,\n",
    "    description='Imag range:',\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    continuous_update=False,\n",
    "    readout_format='.2f',\n",
    "    layout=Layout(width='80%', height='30px'),\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check threading backend in use by NumPy\n",
    "from threadpoolctl import threadpool_info\n",
    "nfo=threadpool_info()\n",
    "print(nfo[0][\"internal_api\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using GPUs at Warwick\n",
    "========================\n",
    "\n",
    "D. Quigley, Computational Techniques 2021\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"https://hexus.net/media/uploaded/2019/8/af64deef-ac8a-47c7-b95f-07baf4658b18.jpg\" width=300 height=300>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"https://hardwareviews.com/wp-content/uploads/2020/06/07104540116l.jpg\" width=300 height=300>\n",
    "    </td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "    <td width=\"500px\">\n",
    "        <center><h3>AMD EPYC 7742 64-core central processing unit (CPU)</h3></center>\n",
    "    </td>\n",
    "    <td width=\"500px\">\n",
    "       <center><h3> Nvidia GeForce 3080 Graphics Processing Unit (GPU) with 8704 \"CUDA cores\"</h3></center>\n",
    "    </td>\n",
    "    \n",
    "</tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GPU Pros?\n",
    "\n",
    "* #### Parallel execution of thousands of threads vs a few dozen (maximum) on a CPU\n",
    "* #### Huge energy efficiency in comparison to the same performance on a traditional CPU cluster\n",
    "\n",
    "## GPU Cons?\n",
    "\n",
    "* ####  Typically no more than 32GB of RAM per device, compared to 2-4GB per CPU core in traditional HPC clusters\n",
    "* ####  Clock rate of around 1-2 GHz vs 3-5 GHz in CPUs, and less work done per clock 'tick'\n",
    "* ####  Bandwidth between device memory and the compute units is fast, but not thousands of times faster than CPUs\n",
    "* ####  Threads are grouped into *warps* which all execute instructions in lockstep\n",
    "* ####  Code must be (re)written to explicitly make use of the GPU capabilities\n",
    "* ####  Only a (growing) subset of your favourite langauge features can execute on a GPU\n",
    "* ####  Not all computational tasks are suitable for GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements for GPU computing\n",
    "\n",
    "* #### [A CUDA-capable Nvidia GPU](https://www.geforce.com/hardware/technology/cuda/supported-gpus)\n",
    "* #### [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)\n",
    "* #### Cheap (£60) desktop cards fine for development and testing, but performance very limited\n",
    "* #### High-end gaming cards (£600-£1,000) can be very powerful for CUDA but lack error correction/detection and features needed for remote monitoring and management in server environments. Poor double precision performance\n",
    "* #### Nvidia manufacture some cards aimed squarely at high performance computing market\n",
    "\n",
    "\n",
    "| GPU card              |  Cores  | Single Precision TFLOPS | Double Precision TFLOPS   | GPU Memory Bandwidth GB/s  |\n",
    "| ----------------------|:--------|:------------------------|:-------------------------:|---------------------------:|\n",
    "| Tesla K20 (2012)        | 2496 | 3.52 | 1.18 |  208 |\n",
    "| Tesla K40 ([Chiron](https://warwick.ac.uk/research/rtp/sc/hpc)) (2014)         | 2880 | 4.23 | 1.43 |  235 |\n",
    "| Telsa K80 ([Tinis](https://warwick.ac.uk/research/rtp/sc/hpc)) (2015)   |  4992   | 5.59 | 1.87                      | 480                        |\n",
    "| Tesla P100 ([Orac](https://warwick.ac.uk/research/rtp/sc/hpc)) (2017)   |  3584   | 8.07 | 4.36                       | 732                        |\n",
    "| RTX 6000 ([Avon](https://warwick.ac.uk/research/rtp/sc/hpc)) (2020)     |  4608   | 16.03 | 0.51 | 672  |\n",
    "| A100 ([Sulis](https://sulis.ac.uk)) (2021) |  6912   | 19.5 | 9.7 | 1,600  |\n",
    "| GeForce 3080 (Gaming)|  8704   | 25.1 | 0.39                     | 760                        |\n",
    "\n",
    "(Quoting [Wikipedia](https://en.wikipedia.org/wiki/Nvidia_Tesla) - TFLOPS shown are without Nvidia Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Programming model\n",
    "\n",
    "* ####  CUDA is the (proprietary) programming model for exploiting Nvidia GPUs\n",
    "* ####  Others exist (e.g. OpenCL) but arguably CUDA has become the de-facto standard and is much easier to learn\n",
    "* ####  CUDA extends the C programming language with GPU features. CUDA Fortran also exists (and is now free via Nvidia HPC SDK)\n",
    "* ####  Various third party tools for other languages - we will focus on [Numba](https://numba.pydata.org/) for Python as it appears closer in spirit to CUDA with compiled languages and so concepts are transferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CUDA and the [Scientific Computing Research Technology Platform (SCRTP)](https://warwick.ac.uk/research/rtp/sc)\n",
    "\n",
    "* #### Managed desktop supports CUDA toolkit 11.1 for machines with GPU cards which support it\n",
    "* #### Check support using `nvidia-smi` in the terminal\n",
    "* #### Most desktop machines are supported. Some spare GeForce 1030 cards are available for machines which can accomodate them\n",
    "* #### 16 GPU nodes each with 3x on Nvidia RTX 6000 hardware in Avon, and 1 OpenPower testbed with 4x Nvidia P100 cards. 30 GPU nodes with 3x Nvidia A100 in Sulis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 1 : Preliminaries\n",
    "\n",
    "### Sanity check of Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some terminology:\n",
    "\n",
    "*  **Host**        : The traditional computer in which our code is running on a CPU with access to host RAM\n",
    "\n",
    "* **CUDA Device** : The GPU card consisting of its own RAM and computing cores (lots of them) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import platform          # So we can figure out where we're running\n",
    "from numba import cuda   # Import python interface to CUDA\n",
    "\n",
    "# Report where we're running\n",
    "print(\"========================================================\")\n",
    "print(\"This notebook is running on \", platform.node())\n",
    "print(\"========================================================\")\n",
    "\n",
    "# Test if CUDA is available. If so report on the devices present \n",
    "if cuda.is_available():  \n",
    "    \n",
    "    # List of CUDA capable devices in this system\n",
    "    for device in cuda.list_devices():       \n",
    "        print(\"Device ID : \", device.id, \" : \", device.name.decode())         \n",
    "    \n",
    "else:\n",
    "    print(\"There doesn't appear to be a CUDA capable device in this system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### Select the the most appropriate device and query its [compute capability](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)\n",
    "\n",
    "* #### [Numba requires a compute capability of 2.0 or higher](https://numba.pydata.org/numba-doc/dev/cuda/overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "my_instance = cuda.select_device(0) # Create a device instance to work with based on device 0 \n",
    "\n",
    "# The compute capability is stored as a tuple (major, minor) so we're good to go if...\n",
    "if my_instance.compute_capability[0] >= 2 and my_instance.compute_capability[1] >= 0 :\n",
    "    print(\"The selected device (\",my_instance.name.decode(),\") has a sufficient compute capability\")\n",
    "else:\n",
    "    print(\"The selected device does not have a sufficient compute capability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Normally we won't need to call ```cuda.select_device()```. The default context will be the fastest GPU in the machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 2 : Libraries\n",
    "\n",
    "### e.g. BLAS (Basic Linear Algebra Subprograms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* #### BLAS is the suite of [Basic Linear Algebra Subprograms](http://www.netlib.org/blas/). \n",
    "* #### These come in three levels, for both real and complex data types \n",
    "\n",
    " - **Level 1** : Vector-vector operations \n",
    "\n",
    " - **Level 2** : Matrix-vector operations\n",
    "\n",
    " - **Level 3** : Matrix-matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cuBLAS\n",
    "\n",
    "* #### The CUDA toolkit includes [cuBLAS](https://developer.nvidia.com/cublas), a GPU-accelerated BLAS implementation\n",
    "\n",
    "* #### [CuPy](https://cupy.dev/) provides a NumPy-like Python interface to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* #### Numpy CPU-based matrix multiplication for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer  # Timer\n",
    "\n",
    "size = 1000\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "A = np.random.rand(size, size)\n",
    "B = np.random.rand(size, size)\n",
    "\n",
    "# Alpha (we'll leave beta as zero)\n",
    "alpha = 1.0\n",
    "\n",
    "# What is the time before we start the operation?\n",
    "t1 = timer()\n",
    "\n",
    "# Perform the operation described above using standard numpy \n",
    "from threadpoolctl import threadpool_limits \n",
    "with threadpool_limits(limits=1, user_api='openmp'):\n",
    "    C_np = alpha * np.matmul(A, B) \n",
    "\n",
    "# What is the time after we finish the operation in milliseconds?\n",
    "t2 = timer()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using numpy took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* #### Using Python interface to cuBLAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp      # Include Numpy-like interface to cuBLAS\n",
    "\n",
    "# Create some square matrices and fill them with random numbers\n",
    "# We do this directly on the device.\n",
    "A_d = cp.random.rand(size, size)\n",
    "B_d = cp.random.rand(size, size)\n",
    "\n",
    "# First event before we call gemm\n",
    "t1 = timer()\n",
    "\n",
    "# Do the multiplication\n",
    "C_cu = cp.matmul(A_d, B_d)\n",
    "\n",
    "# Second event after we call gemm\n",
    "t2 = timer()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Matrix multiplication using cuBLAS took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other libraries\n",
    "\n",
    "Many other GPU accelerated libraries exist. For example...\n",
    "\n",
    "* [cuFFT](https://docs.cupy.dev/en/stable/reference/fft.html?highlight=fft) : Part of the Nvidia toolkit. Fast Fourier Transform calculation.\n",
    "* [cuRAND](https://docs.cupy.dev/en/stable/reference/random.html)      : Part of the Nvidia toolkit. Fast random number generation on GPUs for applications which need large numbers of samples \n",
    "* [Magma](https://icl.cs.utk.edu/magma/) : Dense linear algebra library similar to LaPACK\n",
    "* [cuDNN](https://developer.nvidia.com/cudnn)       : Nvidia Deep Neural Network Library (used by e.g. [TensorFlow](https://www.tensorflow.org/)) \n",
    "* [libgpuarray](http://deeplearning.net/software/libgpuarray/installation.html) : Another tensor manipulation library with [PyGPU](http://deeplearning.net/software/libgpuarray/pyapi/pygpu.html) as the Python interface. Used by [Theano](http://deeplearning.net/software/theano/) \n",
    "\n",
    "These are all available via the environment module system on the SCRTP desktops/workstations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 3 : Kernels\n",
    "\n",
    "### Writing your own functions which execute on the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a kernel?\n",
    "\n",
    "* #### A function which operates on an element of data and is executed by a CUDA thread\n",
    "* #### We launch enough threads such that the operation is performed on all elements\n",
    "* #### Threads are mapped onto our data via a grid of thread blocks\n",
    "* #### Each instance of the function (thread) must be able to identify its location in the grid \n",
    "* #### Kernels are launched/invoked by the host, but run on the GPU\\*\n",
    "\n",
    "(* Newer version of CUDA allow kernels to launch kernels, but numba doesn't support this yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel limitations\n",
    "\n",
    "* #### Can only return data via arguments to the kernel function - no return value\n",
    "* #### Kernels cannot perform input or output - no printing or reading/writing files\n",
    "* #### Exception handling inside kernels is limited (no ```try```/```catch```)\n",
    "* #### Only a subset of the host language (e.g. Python or C) is supported\n",
    "* #### Threads execute in lockstep within *warps* of 32 threads which map onto *multiprocessors* (groups of CUDA cores, e.g. 8, 32 or 128, 192 etc depending on the architecture version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The final limitation might seem unimportant, but this means that anything which causes one thread in a warp to wait (e.g. for data to arrive from memory) will cause all threads to wait. Similary branches, (if statements) are problematic. Each thread must execute both branches and then decide which result to keep. This may subvert traditional expectations of how to optimise code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Thread blocks/grids\n",
    "\n",
    "* #### Threads within a block can make use of some *shared device memory* \n",
    "* #### All threads can read/write to global device memory. \n",
    "* #### There are hardware limitations on the number of threads per block\n",
    "* #### The grid can be 1D, 2D or 3D\n",
    "\n",
    "See https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html for supported features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mandelbrot set\n",
    "\n",
    "* #### Example based on generation of the [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set). Borrowing very heavily here from an [example workbook on GitHub](https://github.com/harrism/numba_examples/blob/master/mandelbrot_numba.ipynb)\n",
    "\n",
    "* #### Coordinates $x$, $y$ are part of the Mandelbrot set if the iterative map\n",
    "  $$ z_{i+1} = z_{i}^{2} + c $$\n",
    "  #### does not diverge when the complex numbers $z_{0} = 0$ and $ c = x + iy$. \n",
    "\n",
    "* #### Represent graphically as number of iterations taken to reach $ |z_{i}| >= 4 $\n",
    "* #### Colour map over the Argand plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### Function to calculate number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mandel(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### Numpy code running on the CPU to generate the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Array to hold the output image - i.e. number of iterations \n",
    "# as an unsigned 8 bit integer\n",
    "image = np.zeros((800, 1600), dtype = np.uint8)\n",
    "\n",
    "# Range over which we want to explore membership of the set\n",
    "rmin = -2.0 ; rmax = 1.0\n",
    "imin = -1.0 ; imax = 1.0\n",
    "\n",
    "# Maximum number of iterations before deciding \"does not diverge\"\n",
    "maxits = 20\n",
    "\n",
    "t1 = timer() # Start timer\n",
    "\n",
    "pixel_size_x = (rmax - rmin) / image.shape[1]\n",
    "pixel_size_y = (imax - imin) / image.shape[0]\n",
    "\n",
    "# This is probably the most non-pythonic way to do this...\n",
    "for index, pixel in np.ndenumerate(image):\n",
    "    real = rmin + index[1] * pixel_size_x\n",
    "    imag = imin + index[0] * pixel_size_y\n",
    "        \n",
    "    image[index[0],index[1]] = mandel(real, imag, maxits)\n",
    "        \n",
    "t2 = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print time taken\n",
    "print(\"Mandelbot created on CPU in : \",1000*(t2-t1),\" milliseconds.\")\n",
    "\n",
    "# Display the image\n",
    "fig = plt.figure(figsize = [10, 10])\n",
    "plt.imshow(image,cmap='RdBu', extent=[rmin, rmax, imin, imax]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* #### For the GPU version we need `mandel` as a function which runs on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the device function mandel_gpu from the function \"mandel\" above\n",
    "mandel_gpu = cuda.jit(device=True)(mandel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### We also need a kernel launched by the host to run on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mandel_kernel(min_x, max_x, min_y, max_y, image, iters):\n",
    "  \"\"\"\n",
    "    GPU kernel invoked on every pixel in an image.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Get the dimensions of the grid from the image device array\n",
    "  dimx = image.shape[1]\n",
    "  dimy = image.shape[0]\n",
    "\n",
    "  # Work out spacing between elements \n",
    "  pixel_size_x = (max_x - min_x) / dimx\n",
    "  pixel_size_y = (max_y - min_y) / dimy\n",
    "\n",
    "  # What elements of the image should this thread operate on?\n",
    "  tx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "  ty = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "\n",
    "  # Coordinates in the complex plane\n",
    "  real = min_x + tx * pixel_size_x\n",
    "  imag = min_y + ty * pixel_size_y \n",
    "    \n",
    "  # Count number of interations needed to diverge and store in the image\n",
    "  if ty < dimy and tx < dimx:\n",
    "      image[ty, tx] = mandel_gpu(real, imag, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* #### The array `image` will be a device array, it exists on the device not the host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### The decorator `@cuda.jit` defines this as a kernel\n",
    "* #### It is compiled to GPU code \"just in time\" (jit)\n",
    "* #### The kernel needs to know which part of the image to calculate\n",
    "  ```python\n",
    "  cuda.threadIdx.x  # Thread index in current block\n",
    "  cuda.blockIdx.x   # Block index\n",
    "  cuda.blockDim.x   # Num Threads in each block\n",
    "  ```\n",
    "  ... plus similar in the y direction (and z if in 3D). \n",
    "  \n",
    "* #### Thread coordinates within the overall grid\n",
    "\n",
    "  ```python\n",
    "  tx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "  ty = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### Check that `tx` and `ty` will have something to operate on \n",
    "  ```python\n",
    "     if ty < dimy and tx < dimx:\n",
    "         image[ty, tx] = mandel_gpu(real, imag, iters)\n",
    "  ```\n",
    "* #### Good practice - host code may specify more threads than pixels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* #### Host code, runs on the CPU and invokes the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The image size above is chosen to map onto a whole number of threadblocks. \n",
    "# IMPORTANT - we normally think of arrays indexed as row, column hence y, x\n",
    "# The tuples specifiying the thread grid dimensions are indexed as x, y\n",
    "threads_per_block = (32, 32) \n",
    "\n",
    "bx = image.shape[1] // threads_per_block[1] + 1\n",
    "by = image.shape[0] // threads_per_block[0] + 1\n",
    "\n",
    "blocks_per_grid = (bx, by)\n",
    "\n",
    "t1 = timer() # Start timer\n",
    "\n",
    "# Copy image to a device array which we will populate in our kernel\n",
    "d_image = cuda.to_device(image)\n",
    "\n",
    "# Launch the kernel, passing the range of x and y to use \n",
    "mandel_kernel[blocks_per_grid, threads_per_block](rmin, rmax, imin, imax, d_image, maxits) \n",
    "\n",
    "# Copy the resulting image back to the host\n",
    "image = d_image.copy_to_host()\n",
    "\n",
    "t2 = timer()  # Stop timer\n",
    "\n",
    "print(\"Mandelbot created on GPU in : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plt.figure(figsize = [9, 9])\n",
    "plt.imshow(image,cmap='RdBu', extent=[rmin, rmax, imin, imax]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ### Function to generate image over a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def update_plot(rrange, irange):\n",
    "\n",
    "    (rmin, rmax) = rrange\n",
    "    (imin, imax) = irange \n",
    "                   \n",
    "    t1 = timer() # Start timer\n",
    "\n",
    "    # Launch the kernel, passing the range of x and y to use \n",
    "    mandel_kernel[blocks_per_grid, threads_per_block](rmin, rmax, imin, imax, d_image, maxits) \n",
    "\n",
    "    # Copy the resulting image back to the host\n",
    "    image = d_image.copy_to_host()\n",
    "\n",
    "    t2 = timer()  # Stop timer\n",
    "    \n",
    "    print(\"Mandelbot created on GPU in : \",1000*(t2-t1),\" milliseconds.\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize = [18, 12]);\n",
    "    plt.imshow(image, cmap='RdBu', aspect='auto', extent=[rmin, rmax, imin, imax]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* ### Explore the real and imaginary range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myplot = interact(update_plot, rrange=real_range, irange=imag_range, continuous_update=True);\n",
    "display(myplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* ####  GPUs provide access to thousands of simple compute cores\n",
    "* ####  GPU accelerated libraries exist, often as drop-in replacements for CPU code\n",
    "* ####  Writing custom kernels can be fairly simple\n",
    "* ####  Significant speedups available for some (but by no means all) problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Final points\n",
    "\n",
    "* #### More examples notebooks at [GitHub/WarwickRSE/gpuschool2018](https://github.com/WarwickRSE/gpuschool2018)\n",
    "* #### Many GPU nodes/servers available in Avon and Sulis via the SCRTP"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
