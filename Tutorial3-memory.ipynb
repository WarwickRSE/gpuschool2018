{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to GPU Programming Summer School\n",
    "========================\n",
    "\n",
    "D. Quigley, University of Warwick\n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial 3: Memory\n",
    "\n",
    "\n",
    "#### You can reasonably consider the material in this session to be \"advanced\". We'll be taking the acceleration we get from a GPU and optimising how memroy is used to get even better speedups.\n",
    "\n",
    "* #### So far we've only explicitly managed device arrays which live in *global memory* - the several GB of RAM on the device\n",
    "\n",
    "* #### We've implicitly used *local memory* - stack variables created automatically for each instance of a kernel, e.g. the loop counters in our Mandelbrot example\n",
    "\n",
    "* #### Global memory is large (relatively speaking) but not \"on-chip\". Reads and writes from/to global memory are slow compared to operating on data already in the multiprocessor registers\n",
    "\n",
    "* #### *Shared memory* is a small quantity of read/write on-chip memory shared by all threads in a block. It can be used as a cache, but the programmer must code for this\n",
    "\n",
    "* #### *Constant memory* is a set by the host, and is read only on the device. It is automatically cached on-chip and so can be used to speed up access to commonly used data\n",
    "\n",
    "* #### Both constant memory and shared memory have size limits which depend on the device and its compute cabability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import what we'll need for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                 # Numpy\n",
    "from numba import cuda, float64    # Cuda and numba float64 datatype\n",
    "\n",
    "import matplotlib.pyplot as plt    # Matplotlib\n",
    "%matplotlib inline                 \n",
    "\n",
    "from timeit import default_timer as timer  # Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following kernel. This evolves a point on a 2D grid forward in time according to a simple finite-difference scheme for numerically solving the diffusion equation.\n",
    "\n",
    "$$ U^{n+1}_{i,j} = U^{n}_{i,j} + D \\delta t\\left[\\frac{ U^{n}_{i-1,j} - 2 U^{n}_{i,j} + U^{n}_{i+1,j} }{{\\delta x}^2} + \\frac{ U^{n}_{i,j-1} - 2 U^{n}_{i,j} + U^{n}_{i,j+1} }{{\\delta y}^2} \\right] $$\n",
    "\n",
    "Here $U$ is some function which obeys the diffusion equation, and $D$ is the diffusion coefficient. Grid spacings are  $\\delta x$ and $\\delta y$ and time is evolved forward in steps of $\\delta t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def diffusion_kernel(D, invdx2, invdy2, dt, d_u, d_u_new):\n",
    "    \"\"\"\n",
    "    Simple kernel to evolve a function U forward in time according to an explicit FTCS\n",
    "    finite difference scheme. Arguments are...\n",
    "    \n",
    "    D       : Diffusion coefficient\n",
    "    invdx2  : 1/(dx^2) where dx is the grid spacing in the x direction \n",
    "    invdy2  : 1/(dy^2) where dy is the grid spacing in the y direction\n",
    "    dt      : time step\n",
    "    d_u     : Device array storing U at the current time step\n",
    "    d_u_new : Device array storing U at the next time step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Which row and column on the simulation grid should this thread use\n",
    "    row = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    col = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Check that the index lies inside the grid\n",
    "    if row < d_u.shape[0] and col < d_u.shape[1]:\n",
    "    \n",
    "        # Neighbour cells using period boundary conditions\n",
    "        up   = (row + 1)%d_u.shape[0]\n",
    "        down = (row - 1)%d_u.shape[0]\n",
    "    \n",
    "        left  = (col - 1)%d_u.shape[1]\n",
    "        right = (col + 1)%d_u.shape[1]\n",
    "        \n",
    "        # Compute second derivatives of u w.r.t. x and y\n",
    "        d2udx2 = (d_u[row,left]  - 2.0*d_u[row,col] + d_u[row,right])*invdx2\n",
    "        d2udy2 = (d_u[down,col]  - 2.0*d_u[row,col] + d_u[up,col])*invdy2\n",
    "    \n",
    "        # Populate u_new with the time-evolved function\n",
    "        d_u_new[row, col] = d_u[row, col] + D * dt * ( d2udx2 + d2udy2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need some parameters/data to play with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "dim = 960\n",
    "c = np.zeros((dim, dim))\n",
    "\n",
    "# Fill the middle of the grid with a concentration of 1.0\n",
    "for irow in range(c.shape[0] // 4, 3*c.shape[0] // 4):\n",
    "    for icol in range(c.shape[1] // 4, 3*c.shape[1] // 4):\n",
    "        c[irow, icol] = 1.0\n",
    "        \n",
    "# We want this to represent a square domain spanning 0 -> 1 in each direction\n",
    "domain = [0.0, 1.0, 0.0, 1.0]\n",
    "                                        \n",
    "D = 1.0  # Diffusion coefficient\n",
    "\n",
    "x_spacing = 1.0/float(c.shape[0])\n",
    "y_spacing = 1.0/float(c.shape[1])\n",
    "\n",
    "# Store spacing as inverse square to avoid repeated division\n",
    "inv_xspsq = 1.0/(x_spacing**2)\n",
    "inv_yspsq = 1.0/(y_spacing**2)\n",
    "\n",
    "# Satisfy stability condition \n",
    "time_step = 0.25*min(x_spacing,y_spacing)**2\n",
    "                \n",
    "# Plot \n",
    "fig = plt.figure(figsize = [6, 6])\n",
    "plt.imshow(c,cmap='jet',extent=domain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got everything we need to use `diffusion_kernel` to evolve this initial condition forward in time on the GPU. Let's time this and examine the output. Note that I've been careful here to ensure that the grid size is an exact multiple of the number of threads in a block.\n",
    "\n",
    "Note also that I have to call ```cuda.synchronise()``` before stopping the timer. In previous examples we've only stopped the timer after making sure the data is available by copying it back to the host, and the copy won't start until all the kernels are finished. Launching of kernels is *asynchronous* - the code will reach the line ```t2 = timer``` after the final time kernels are launched, not when they complete. Since there's no requirement on the data calculated before we reach that statement we have to block progress until all threads complete manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this array to the device, and create a new device array to hold updated value\n",
    "d_c = cuda.to_device(c)\n",
    "d_c_new = cuda.device_array(c.shape, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = (32, 32)\n",
    "blocks_per_grid   = (dim//32, dim//32)\n",
    "\n",
    "t1 = timer()  # Start timer\n",
    "\n",
    "# Evolve forward 2000 steps\n",
    "for step in range(2000):\n",
    "\n",
    "    # Launch the kernel    \n",
    "    diffusion_kernel[blocks_per_grid,threads_per_block](D, inv_xspsq, inv_yspsq, time_step, d_c, d_c_new)\n",
    "\n",
    "    # Swap the identit of the old and new device arrays\n",
    "    d_c, d_c_new = d_c_new, d_c\n",
    "    \n",
    "cuda.synchronize()  # make sure all threads finish before stopping timer\n",
    "       \n",
    "t2 = timer()\n",
    "print(\"Simulation using simple kernel took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the current concentration from the device to the host\n",
    "c = d_c.copy_to_host()\n",
    "\n",
    "# Display the current concentration profile\n",
    "fig = plt.figure(figsize = [6, 6])\n",
    "plt.imshow(c,cmap='jet',extent=[0.0, 1.0, 0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this kernel is okay, but we can do better.\n",
    "\n",
    "Consider the part of the kernel which reads from the device array d_u.\n",
    "\n",
    "```python\n",
    "        # Compute second derivatives of u w.r.t. x and y\n",
    "        d2udx2 = (d_u[row,left]  - 2.0*d_u[row,col] + d_u[row,right])*invdx2\n",
    "        d2udy2 = (d_u[down,col]  - 2.0*d_u[row,col] + d_u[up,col])*invdy2\n",
    "    \n",
    "        # Populate u_new with the time-evolved function\n",
    "        d_u_new[row, col] = d_u[row, col] + D * dt * ( d2udx2 + d2udy2) \n",
    "```\n",
    "Each instance of the kernel (i.e. each thread) needs to fetch five entries from the array ```d_u``` in device global memory.  Loads from memory are slow compared to compute operations, so there's a good chance these loads account for a significant fraction of the time measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside - not everyone will be familiar with how 2D data is stored. By default numpy (and hence numba) 2D arrays are arranged in the same way as in C.\n",
    "\n",
    "![](C2d.png)\n",
    "\n",
    "(The alternative is column-major ordering as used in Fortran).\n",
    "\n",
    "This layout means that the column index varies most rapidly as we scan through the memory, i.e. neighbouring elements on the same row as `d_u[row, col]` are near each other in memory and might be all loaded together as part of the same fetch operation, reducing the number required to 3. Neighbouring elements in the same column but on different rows will be far away in memory and will require additional fetch operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve performance by recognising that other threads within the same thread block will also need some of these neighbouring entries in the array. If those threads can *share* the data that is loaded from global memory then we'd cut down on the number of loads needed. This is what shared memory is used for - a local (but limited in size) on-chip cache where we can store data which multiple threads within the block will need.\n",
    "\n",
    "Consider the following kernel. Here threads in blocks of 32 x 32 each load one element from global memory into shared memory. To avoid needing data outside of the shared memory array, only the interior 30 x 30 threads perform computation.\n",
    "\n",
    "In the lingo this is known as *coalescing* memory access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def diffusion_kernel_shared(D, invdx2, invdy2, dt, d_u, d_u_new):\n",
    "    \"\"\"\n",
    "    Kernel to evolve a function U forward in time according to an explicit FTCS\n",
    "    finite difference scheme. Shared memory is used. Arguments are...\n",
    "    \n",
    "    D       : Diffusion coefficient\n",
    "    invdx2  : 1/(dx^2) where dx is the grid spacing in the x direction \n",
    "    invdy2  : 1/(dy^2) where dy is the grid spacing in the y direction\n",
    "    dt      : time step\n",
    "    d_u     : Device array storing U at the current time step\n",
    "    d_u_new : Device array storing U at the next time step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shared array large enough to store 30x30 block + \"halo\" of boundary neighbours\n",
    "    # N.B. the size of the shared array is set at compile time, but see also \n",
    "    # https://stackoverflow.com/questions/30510580/numba-cuda-shared-memory-size-at-runtime\n",
    "    u_sh = cuda.shared.array((32, 32), dtype=float64)\n",
    "    \n",
    "    # Row and column in global matrix - 32 x 32 grid including halo\n",
    "    row = cuda.threadIdx.y + cuda.blockIdx.y * ( cuda.blockDim.y - 2 ) - 1\n",
    "    col = cuda.threadIdx.x + cuda.blockIdx.x * ( cuda.blockDim.x - 2 ) - 1\n",
    "    \n",
    "    # Row and column in shared memory tile\n",
    "    sh_row = cuda.threadIdx.y\n",
    "    sh_col = cuda.threadIdx.x\n",
    "    \n",
    "    # Apply periodic boundary conditions\n",
    "    row = row%d_u.shape[0]\n",
    "    col = col%d_u.shape[1]\n",
    "    \n",
    "    # Copy from device memory to shared memory\n",
    "    u_sh[sh_row, sh_col] = d_u[row, col]\n",
    "    \n",
    "    # Do not proceed until all threads reach this point\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Only threads which belong to the interior 30 x 30 grid compute\n",
    "    # (The other 32^2 - 30^30 = 124 threads do nothing)\n",
    "    if sh_row > 0 and sh_row < 31 and sh_col > 0 and sh_col < 31:\n",
    "        \n",
    "        left  = sh_col - 1\n",
    "        right = sh_col + 1\n",
    "        \n",
    "        up   = sh_row + 1\n",
    "        down = sh_row - 1 \n",
    "        \n",
    "        # Compute second derivatives of u w.r.t. x and y\n",
    "        d2udx2 = (u_sh[sh_row, left]  - 2.0*u_sh[sh_row, sh_col] + u_sh[sh_row, right])*invdx2\n",
    "        d2udy2 = (u_sh[down, sh_col]  - 2.0*u_sh[sh_row, sh_col] + u_sh[up, sh_col])*invdy2\n",
    "    \n",
    "        # Populate u_new with the time-evolved function\n",
    "        d_u_new[row, col] = u_sh[sh_row, sh_col] + D * dt * ( d2udx2 + d2udy2)  \n",
    "    \n",
    "    u_sh = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each thread now only performs one load from global memory, but we'll need to launch more blocks to cover the grid.\n",
    "\n",
    "Let's see if this works. Reset to the initial condition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "dim = 960\n",
    "c = np.zeros((dim, dim))\n",
    "\n",
    "# Fill the middle of the grid with a concentration of 1.0\n",
    "for irow in range(c.shape[0] // 4, 3*c.shape[0] // 4):\n",
    "    for icol in range(c.shape[1] // 4, 3*c.shape[1] // 4):\n",
    "        c[irow, icol] = 1.0\n",
    "        \n",
    "d_c = cuda.to_device(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and launch the new kernel, being careful to use more blocks than before to cover the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = (32, 32)\n",
    "blocks_per_grid   = (dim//30, dim//30)\n",
    "\n",
    "t1 = timer()\n",
    "\n",
    "# Evolve forward 2000 steps\n",
    "for step in range(2000):\n",
    "\n",
    "    # Launch the kernel    \n",
    "    diffusion_kernel_shared[blocks_per_grid,threads_per_block](D, inv_xspsq, inv_yspsq, time_step, d_c, d_c_new)\n",
    " \n",
    "    # Swap the identity of the old and new device arrays\n",
    "    d_c, d_c_new = d_c_new, d_c\n",
    "    \n",
    "cuda.synchronize()  # make sure all threads finish before stopping timer\n",
    "    \n",
    "t2 = timer()\n",
    "\n",
    "print(\"Simulation using shared memory kernel took : \",1000*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the current concentration from the device to the host\n",
    "c = d_c.copy_to_host()\n",
    "\n",
    "# Display the current concentration profile\n",
    "fig = plt.figure(figsize = [6, 6])\n",
    "plt.imshow(c,cmap='jet',extent=[0.0, 1.0, 0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing this resulted in a saving of approximately 15%, despite the fact that we're not using all of the threads in a block to do computation. The saving on loads from global memory has more than offset that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* How do these kernels perform relative to the corresponding CPU implementation? Use the Tinis GPU node to gather your benchmarks and remember to take averages excluding compilation.\n",
    "\n",
    "* How does the benefit gained from using shared memory to eliminate redundant loads vary with the size of the grid and the number of timesteps simulated? I've been a bit lazy with my code, so you'll need to make sure all grid sizes are a multiple of both 32 and 30.\n",
    "\n",
    "* (Advanced) How does the balance between use of shared memory and threads idle during computation change when moving from 2D to 3D?\n",
    "\n",
    "* (Very advanced) Can you create a kernel for this problem which exploits the use of shared memory without leaving any threads idle during computation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic operations\n",
    "\n",
    "Shared memory can also be used if wanting multiple threads to read/write to the same variable without going to global memory. This raises the issue of contention and possible race conditions.\n",
    "\n",
    "Consider the following which sums over the elements in each row of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def sum_row(d_a, d_sum):\n",
    "    \"\"\"Given a device array a, calculate the sum over elements in each row.\"\"\"\n",
    "\n",
    "    # Get row and column of current element\n",
    "    row = cuda.threadIdx.y    + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    column = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    if (row < d_a.shape[0]):\n",
    "        if column == 0 :                   \n",
    "            d_sum[row] = 0.0 # First thread in row initialises sum\n",
    "    \n",
    "        if column < d_a.shape[1]:\n",
    "            #cuda.atomic.add(d_sum, row , d_a[row, column])\n",
    "            d_sum[row] += d_a[row, column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on a trivial array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random 3 x 2 array\n",
    "my_array = np.random.rand(3, 2)\n",
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sum over elements in each row (axis = 1 means sum over columns)\n",
    "np.sum(my_array, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same calculation using our new kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to device and create new array for output\n",
    "d_my_array = cuda.to_device(my_array)\n",
    "d_my_sum   = cuda.device_array(3, dtype=np.float64)\n",
    "\n",
    "# Launch a single thread block of 2 x 3 threads\n",
    "sum_row[(1, 1), (2, 3)](d_my_array, d_my_sum)\n",
    "\n",
    "# Copy result back and print it\n",
    "my_sum = d_my_sum.copy_to_host()\n",
    "my_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panic - this hasn't given us the correct result. What could be wrong? \n",
    "\n",
    "The problem is a *race condition*. There is nothing stopping threads in columns other than column 0 from adding in their contribution to the sum before it has been initialised. \n",
    "\n",
    "Similarly thread A must read the current value of the sum before adding its own contribution. If another thread B changes the sum after A has read the old value, but before A writes the result of its sum, A will be working with a *stale* value and get the wrong result.\n",
    "\n",
    "We can take steps to fix this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def sum_row_v2(d_a, d_sum):\n",
    "    \"\"\"Given a device array a, calculate the sum over elements in each row.\"\"\"\n",
    "\n",
    "    # Get row and column of current element\n",
    "    row = cuda.threadIdx.y    + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    column = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    if (row < d_a.shape[0]):\n",
    "        if column == 0 :                   \n",
    "            d_sum[row] = 0.0 # First thread in row initialises sum\n",
    "    \n",
    "    # No thread can pass this point until all threads reach this statement\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    if (row < d_a.shape[0]):\n",
    "        if column < d_a.shape[1]:\n",
    "            \n",
    "            # Add to element 'row' of array d_sum\n",
    "            cuda.atomic.add(d_sum, row , d_a[row, column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used an *atomic operation* - in this case the operation 'sum'. Atomics prevent more than one thread at a time being able to read/write from a particular memory location. They act like a thread lock. Other operations than sum are available. See [the documentation](https://numba.pydata.org/numba-doc/dev/cuda/intrinsics.html).\n",
    "\n",
    "We've also use ```cuda.syncthreads()``` to ensure no thread writes to the sum before it is initialised.\n",
    "\n",
    "Let's check that we get the correct answer this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to device and create new array for output\n",
    "d_my_array = cuda.to_device(my_array)\n",
    "d_my_sum   = cuda.device_array(3, dtype=np.float64)\n",
    "\n",
    "# Launch a single thread block of 2 x 3 threads\n",
    "sum_row_v2[(1, 1), (2, 3)](d_my_array, d_my_sum)\n",
    "\n",
    "# Copy result back and print it\n",
    "my_sum = d_my_sum.copy_to_host()\n",
    "my_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. Be aware that use of atomics slows code down drastically. They effectively serialise the operations they perform. \n",
    "\n",
    "We can also use atomics on shared memory if we want to reduce access to global memory, but we need to be aware that shared memory only persists for the lifetime of the thread block and so any result must be written back to global memory once computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def sum_row_v3(d_a, d_sum):\n",
    "    \"\"\"Given a device array a, calculate the sum over elements in each row.\"\"\"\n",
    "\n",
    "    # Get row and column of current element\n",
    "    row = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    column = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Create shared memory array to use for summation\n",
    "    sum_sh = cuda.shared.array(3, dtype=float64)\n",
    "    \n",
    "    if (row < d_a.shape[0]):\n",
    "        if column == 0 :                   \n",
    "            sum_sh[row] = 0.0 # First thread in row initialises sum\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    if (row < d_a.shape[0]):\n",
    "        if column < d_a.shape[1]:\n",
    "            \n",
    "            # Add to element 'row' of array sum_sh. Note that we \n",
    "            # don't need to read from global memory anymore\n",
    "            cuda.atomic.add(sum_sh, row , d_a[row, column])\n",
    "            \n",
    "    cuda.syncthreads()\n",
    "           \n",
    "    # Write result to global memory\n",
    "    if (row < d_a.shape[0]):        \n",
    "        if column == 0 : d_sum[row] = sum_sh[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this still works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to device and create new array for output\n",
    "d_my_array = cuda.to_device(my_array)\n",
    "d_my_sum   = cuda.device_array(3, dtype=np.float64)\n",
    "\n",
    "# Launch a single thread block of 2 x 3 threads\n",
    "sum_row_v3[(1, 1), (2, 3)](d_my_array, d_my_sum)\n",
    "\n",
    "# Copy result back and print it\n",
    "my_sum = d_my_sum.copy_to_host()\n",
    "my_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "* The kernel used in ```sum_row_v3``` above will only work as advertised if the array we want to sum over fits within a single thread block. Create a new kernel which will work for arrays of any size and test your result against the sum method of a numpy array on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Constant memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final example we'll do something a bit more substantial for which we might expect to see a significant benefit from using a GPU. Imagine we have a function of three variables $f(x,y,z)$ which is defined as a sum over Gaussians centered on $P$ different points $\\{x_{i},y_{i},z_{i}\\}$. We are only interested in the function between -10 and +10 in each direction.\n",
    "\n",
    "$$f(x,y,z) = \\sum_{i=1}^{P}A\\exp\\left\\{-w\\left[\\left(x-x_{i}\\right)^2+\\left(y-y_{i}\\right)^2+\\left(z-z_{i}\\right)^2\\right]\\right\\}$$\n",
    "\n",
    "Our goal is to integrate out (numerically) one of the dimensions and obtain a two-dimensional function\n",
    "\n",
    "$$g(x,y) = \\int_{-10}^{10}\\exp\\left[f(x,y,z)\\right] dz.$$\n",
    "\n",
    "To compute this function on a $N_{grid} \\times N_{grid}$ mesh in the $x,y$ plane, we must perform the above integration over $z$ at each point. For simplicity we will integrate numerically over $N_{grid}$ points in the $z$ direction using the trapezoidal rule. We must therefore evaluate the function a total of $N_{grid}^3 $ times which quickly becomes expensive as the size of the grid increases.\n",
    "\n",
    "Exactly this problem can arise in molecular simulation when attempting to create a two-dimensional probability density plot from a reconstructed free energy. For anyone intersted, $f(x,y,z)$ is a free energy reconstructed from the points $\\{x_{i},y_{i},z_{i}\\}$ visited during a molecular dynamics simulation. $x,y$ and $z$ are themselves functions of atomic coordinates, such as the end-to-end distance of a polymer, or the number of hydrogen bonds in a system. \n",
    "\n",
    "NOTE - if this code looks like the least pythonic thing you've ever seen there's a reason for that. It has been translated line by line from an existing C code I use in my reseach.\n",
    "\n",
    "We'll start by running a C implementation of this code which runs on the CPU. The file ```quadrature_CPU.c``` should exist in the same directory you're running this notebook from. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will be executed as terminal commands due to the \"!\" at the start\n",
    "! gcc quadrature_cpu.c -o quad_cpu -lm -O3\n",
    "! ./quad_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a function which evaluates $f(x,y,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "\n",
    "def func(x, y, z, d_points, count, height, width):\n",
    "    \"\"\"\n",
    "    This function evaluates f(x,y,z)\n",
    "    \"\"\"\n",
    "    \n",
    "    tot = 0.0\n",
    "\n",
    "    for i in range(count):\n",
    "        arg  = (x-d_points[i,0])**2\n",
    "        arg += (y-d_points[i,1])**2\n",
    "        arg += (z-d_points[i,2])**2\n",
    "        tot += height*m.exp(-width*arg)\n",
    "    \n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the above code into a device function\n",
    "func_gpu = cuda.jit(device=True)(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel we use will integrate over z for each x, y point in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def integrate1D(min_coord, max_coord, N, d_a, d_arrpoint, num_points, gauss_height, gauss_width):\n",
    "    \"\"\"\n",
    "    This is our kernel. for a point on our two-dimension grid\n",
    "    integrate over the function in the third dimension.\n",
    "    kernels must be global, visible to the host and device.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Which element on the 2D grid does this instance \n",
    "    # of the kernel need to compute?\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    if col < N and row < N:\n",
    "\n",
    "        # Compute grid spacing and current x and y value\n",
    "        delta = (max_coord - min_coord)/(N - 1.0)\n",
    "\n",
    "        x     = min_coord + float(col) * delta\n",
    "        y     = min_coord + float(row) * delta\n",
    "\n",
    "        # Integrate along z using trapezoidal rule\n",
    "        g_loc = 0.0\n",
    "        z     = min_coord\n",
    "        f     = func_gpu(x, y, z, d_arrpoint, num_points, gauss_height, gauss_width)\n",
    "        p_old = m.exp(f)\n",
    "        \n",
    "        for k in range(1, N):\n",
    "            z += delta\n",
    "            f = func_gpu(x, y, z, d_arrpoint, num_points, gauss_height, gauss_width)\n",
    "            p_new  = m.exp(f)\n",
    "            g_loc += delta * 0.5 * (p_old + p_new)\n",
    "            p_old  = p_new\n",
    "\n",
    "        # Store at the appropriate location in g\n",
    "        d_a[row, col] = g_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Gaussians\n",
    "P = 500\n",
    "\n",
    "# Number of points on the grid in each dimension\n",
    "Ngrid = 128\n",
    "\n",
    "# Width and height of Gaussians\n",
    "A = 0.1; w = 0.2\n",
    "\n",
    "# Extend of grid\n",
    "grid_min = -10.0\n",
    "grid_max =  10.0\n",
    "\n",
    "# Create P random points in 3 dimension representing the Gaussian centres\n",
    "np.random.seed(12072018)  # Make sure results are reproducible\n",
    "points = (grid_max - grid_min) * np.random.rand(P, 3) + grid_min\n",
    "\n",
    "# 32 x 32 threads in a block\n",
    "threads_per_block = (32, 32)\n",
    "blocks_per_grid = (Ngrid//32, Ngrid//32)\n",
    "\n",
    "# Copy this to the device\n",
    "d_points = cuda.to_device(points)\n",
    "\n",
    "# Allocate memory on the device for the array holding g\n",
    "d_g = cuda.device_array((Ngrid, Ngrid), dtype=np.float)\n",
    "\n",
    "t1 = timer()\n",
    "\n",
    "# Launch the kernel\n",
    "integrate1D[blocks_per_grid, threads_per_block](grid_min, grid_max, Ngrid, d_g, d_points, P, A, w)\n",
    "\n",
    "cuda.synchronize()  # make sure all threads finish before stopping timer\n",
    "\n",
    "t2 = timer()\n",
    "\n",
    "print(\"Calculation of g on the GPU took : \",1000.0*(t2-t1),\" milliseconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy g to the host\n",
    "g = d_g.copy_to_host()\n",
    "\n",
    "# Show the result\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.imshow(g,cmap='jet', extent=[grid_min, grid_max, grid_min, grid_max]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have been pretty speedy, but again we can do better. \n",
    "\n",
    "The array which holds the points needed by every call to ```func_gpu```, and that function is called many times by each thread. That's a lot of loads from global memory. Since these points don't change, we should use constant memory for these data instead.\n",
    "\n",
    "Unfortunately, numba doesn't quite seem to support defining arrays in constant memory at runtime. So we'll have to illustrate the speedup from this using CUDA C code instead. \n",
    "\n",
    "In CUDA C, we would declare the points we want to store in constant memory as a global array (file scope).\n",
    "\n",
    "```C\n",
    "// Number of points P                                                                                                                                    \n",
    "#define P 500\n",
    "\n",
    "// Array of points at which Gaussians are located. This                                                                                                  \n",
    "// is now a static array in constant memory                                                                                                              \n",
    "__constant__ double points[3*P];\n",
    "```\n",
    "\n",
    "In the host code, we'll create the random points and copy them to the device.\n",
    "\n",
    "```C\n",
    "  // Allocate memory for points on the host                                                                                                              \n",
    "  tmpPoints = (double *)malloc(3*P*sizeof(double));\n",
    "\n",
    "  // Populate tmpPoints with random numbers between -10 and 10                                                                                           \n",
    "  for (i=0;i<P;i++) {\n",
    "    for (j=0;j<3;j++) {\n",
    "      x = rand()/(double)RAND_MAX;\n",
    "      tmpPoints[3*i+j] = gridMin + (gridMax-gridMin)*x;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Copy from tmpPoints on the host to points on the device                                                                                             \n",
    "  cudaMemcpyToSymbol(points, tmpPoints, 3*P*sizeof(double));\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "We can test the speed of this implementation by compiling ```quadrature_gpu.cu``` with the Nvidia C compiler ```nvcc```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc quadrature_gpu.cu -o quad_gpu -lm -O3\n",
    "! ./quad_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the timer I'm using in my C code only has 1 millicond resolution. \n",
    "\n",
    "This is about twice as fast at the implementation in python above, but we can't guarantee that all of this is due to use of constant memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Numba hasn't let us reach quite the speed of the CUDA C implementation of this code due to the restrictions on constant memory. Can you think of a way of getting some speedup using shared memory instead? Test this and benchmark on the Tinis GPU nodes.\n",
    "\n",
    "* (C programmers) is the CUDA C version still faster than our python version above if not using constant memory? I've made this easy for you by leaving some commented out lines in the source code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
